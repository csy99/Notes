# 自编码器

这个是NLP专题，我们这里主要讲循环神经网络的子编码器。作用是确定变量的名称，为接下来讲注意力机制做铺垫。自编码器本身在这里讲得不是很清楚，有兴趣了解更多的同学可以移步数学专题中的[信息论]()。

## 编码器

把一个不定长的输入序列变换成一个定长的背景变量$c$，并在该背景变量中编码输入序列信息。编码器可以使用循环神经网络。

在时间步$t$，循环神经网络将输入的特征向量$x_t$和上个时间步的隐藏状态$h_{t−1}$变换为当前时间步的隐藏状态$h_t$。
$$
h_t = f(x_t, h_{t-1})
$$
接下来，编码器通过自定义函数$q$将各个时间步的隐藏状态变换为背景变量
$$
c=q(h_1,...,h_T)
$$
例如，我们可以将背景变量设置成为输入序列最终时间步的隐藏状态$h_T$。

以上描述的编码器是一个单向的循环神经网络，每个时间步的隐藏状态只取决于该时间步及之前的输入子序列。我们也可以使用双向循环神经网络构造编码器。在这种情况下，编码器每个时间步的隐藏状态同时取决于该时间步之前和之后的子序列（包括当前时间步的输入），并编码了整个序列的信息。

## 解码器

解码器的工作就是尝试将编码器输出的向量还原成为原序列。如果不是还原成原序列，而是生成新序列的，则是seq2seq模型，应用有机器翻译。经过压缩之后不可避免的会出现信息的损失，我们需要尽量将这种损失降低（方法是设置合理的中间向量大小和经过多次训练迭代的编码-解码器）。

对每个时间步$t′$，解码器输出$y_{t′}$的条件概率将基于之前的输出序列$y_1,...,y_{t′−1}$和背景变量$c$（所有时间步共用），即$P(y_{t′}∣y_1,...,y_{t′−1},c)$。同时还要考虑上一时间步的隐藏状态$s_{t′−1}$。
$$
s_{t'} = g(y_{t′−1}, c, s_{t'-1})
$$


# 注意力机制

解码器在生成输出序列中的每一个词时可能只需利用输入序列某一部分的信息。为了让NN在不同时间步能够根据信息的有用程度分配权重，我们引入注意力机制。对于普通的解码器，我们每一个时间步可以使用相同的背景变量。而在引入注意力机制之后，我们必须给每一个时间步不同的背景变量。
$$
s_{t'} = g(y_{t′−1}, c_{t'}, s_{t'-1})
$$

### 计算背景变量

<img src="https://i.postimg.cc/ryHV9whR/attention.png" height=200>

上图描绘了注意力机制如何为解码器在时间步2计算背景变量。首先，函数$a$根据解码器在时间步1的隐藏状态和编码器在各个时间步的隐藏状态计算softmax运算的输入。softmax运算输出概率分布并对编码器各个时间步的隐藏状态做加权，从而得到背景变量。
$$
c_{t'} = \sum_{t=1}^T \alpha_{t't}h_t
$$
其中，给定$t'$时，权重$\alpha_{t't}$在$t \in [1, T]$的值是一个概率分布。可以使用softmax计算该分布
$$
\alpha_{t't} = \frac {exp(e_{t't})} {\sum_{k=1}^T exp(e_{t'k})} 
$$






















# Reference

- [Dive Into Deep Learning](http://zh.gluon.ai/chapter_recurrent-neural-networks/rnn.html)，第10章
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762), Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin

  