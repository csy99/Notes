# 引言

我们通常使用RNN处理文本。但是RNN也有其缺点，例如对没有前文的短语的处理能力较弱；另外，在形成文本向量时，通常会很好地捕捉最后一个词的表示，但无法很好捕捉之前的词的表示。我们只会在最后一步使用softmax。

<img src="https://i.postimg.cc/nrLkKmyJ/last-word-vec.png" height=200>

在上例中，句向量基本跟最后一个词的词向量相同。

既然存在这个问题，我们何不对每个固定长度的短语生成一个向量表示？我们不用在意该短语是否具有语法意义。这个任务使用CNN最合适不过。



# textCNN

### 一维卷积层

我们可以将文本当作一维图像，从而可以用一维卷积神经网络来捕捉临近词之间的关联。与二维卷积层一样，一维卷积层使用一维的互相关运算。在一维互相关运算中，卷积窗口从输入数组的最左方开始，按从左往右的顺序，依次在输入数组上滑动。当卷积窗口滑动到某一位置时，窗口中的输入子数组与核数组按元素相乘并求和，得到输出数组中相应位置的元素。

<img src="https://i.postimg.cc/zDcrBK4W/text-CNN-1d.png" height=100>

多输入通道的一维互相关运算也与多输入通道的二维互相关运算类似：在每个通道上，将核与相应的输入做一维互相关运算，并将通道之间的结果相加得到输出结果。由二维互相关运算的定义可知，多输入通道的一维互相关运算可以看作单输入通道的二维互相关运算。如下图所示，我们也可以将上图中多输入通道的一维互相关运算以等价的单输入通道的二维互相关运算呈现。这里核的高等于输入的高，即词向量的维度。

<img src="https://i.postimg.cc/wvY1nnQq/text-CNN-2d.png" height=120>

上面的例子可以看做只使用了一个核。使用一个核会产生一个句向量，我们也可以使用多个核，从多个维度对句子进行表示。

<img src="https://i.postimg.cc/FR0RPnQb/text-CNN-multi-kernels.png" height=330>

图中右半部分就是句子的向量表示。每一列是一个表示。因为我们使用了三个核，所以有三个向量表示。

### 时序最大池化层

textCNN中使用的时序最大池化（max-over-time pooling）层实际上对应一维全局最大池化层：假设输入包含多个通道，各通道由不同时间步上的数值组成，各通道的输出即该通道所有时间步中最大的数值。因此，时序最大池化层的输入在各个通道上的时间步数可以不同。

为提升计算性能，我们常常将不同长度的时序样本组成一个小批量，并通过在较短序列后附加特殊字符（如0）令批量中各时序样本长度相同。这些人为添加的特殊字符当然是无意义的。由于时序最大池化的主要目的是抓取时序中最重要的特征，它通常能使模型不受人为添加字符的影响。

<img src="https://i.postimg.cc/MHRFfVVp/max-pooling-over-time.png" height=200>

经过试验证明，max pooling的效果一般要比average pooling的好。其中一个原因是NLP的特征非常稀疏。比如我们想对影评进行情感分析。每个影评中表示积极的词语占比不大，影评中充斥了大量其他信息，比如对电影的描述。使用average pooling会弱化特征。所以max pooling相对而言有更好的效果。

### 局部最大池化层

仍然使用上面的例子，介绍一下局部最大池化。这里的步长(stride)为2。时序最大池化层会对一个通道所有时间步做池化，而局部最大池化层只对特定长度的时间跨度做池化。

不过用处并不太明显。

<img src="https://i.postimg.cc/Y9Q613nZ/local-max-pooling.png" height=300>

### k最大池化

这个也是最大池化的一个变种。在每个通道除了保留最大值以外，会一共保留最大的k个值。

<img src="https://i.postimg.cc/XqQy8DwH/k-max-pooling.png" height=200>



# 文本分类

### 主旨概述

为了简化模型，方便理解。我们这里使用与Yoon Kim的论文稍微不同的标记。

假设输入的文本序列由$n$个词组成，每个词用$d$维的词向量表示。那么输入样本的宽为$n$，高为1，输入通道数为$d$。

计算步骤如下：

1. 定义多个一维卷积核，并使用这些卷积核对输入分别做卷积计算。宽度不同的卷积核可能会捕捉到不同个数的相邻词的相关性。
2. 对输出的所有通道分别做时序最大池化，再将这些通道的池化输出值连结为向量。
3. 通过全连接层将连结后的向量变换为有关各类别的输出。这一步可以使用丢弃层应对过拟合。

下面这个例子解释了textCNN的设计。假设我们需要进行情感分析任务。这里的输入是一个有11个词的句子，每个词用6维词向量表示。因此输入序列的宽为11，输入通道数为6。给定2个一维卷积核，核宽分别为2和4，输出通道数分别设为4和5。因此，一维卷积计算后，4个输出通道的宽为11−2+1=10，而其他5个通道的宽为11−4+1=8。尽管每个通道的宽不同，我们依然可以对各个通道做时序最大池化，并将9个通道的池化输出连结成一个9维向量。最终，使用全连接将9维向量变换为2维输出，即正面情感和负面情感的预测。

<img src="https://i.postimg.cc/RhNxdZJN/text-CNN-example.png" height=400>

### 模型介绍

目标是文本分类。主要是情感分析（积极、消极），也可以完成一些根据问题内容进行分类的任务。

在Yoon Kim的论文中，词向量$x_i \in R^k$，一个文本由$n$个词组成，向量由词向量拼接形成。核(convolutional filter)使用$w \in R^{hk}$，窗口大小是$h$。注意此处核是一个向量，而不是一个矩阵。可以想象成将矩阵拉平。核的大小可以是2-4。

计算文本特征（以一个通道为例）
$$
c_i = f(w^Tx_{i:i+h-1} + b)
$$
对于某个特定文本，所有窗口长度为h的h-mer包括$\{x_{1:h}, x_{2:h+1},...,x_{n-h+1:n}\}$。所以组成的文本向量为$c = [c_1, c_2, ..., c_{n-h+1}]$。

经过池化有
$$
\hat c = max(c)
$$
在模型中使用了dropout进行正则化，作者认为对结果有大概5%的提升。另外，作者对每一类的权重向量的模长进行了规定（softmax后的一行）。从理论上来说，可能避免了一些溢出风险。
$$
||W_c^{(S)}|| = s
$$
这个规定并不是很常见。

### 模型参数

激活函数：ReLU

窗口大小：h=3,4,5

通道数：100

丢弃率Dropout：p=0.5

L2模长限制：s=3

批大小：B=50

词向量大小：k=300，使用word2vec预训练

### 模型比较

在原paper中，作者训练了如下几种模型进行对比。

- CNN-rand：基线模型，词向量需要从头进行训练
- CNN-static：静态模型，直接使用了经过word2vec预训练后的词向量
- CNN-non-static：非静态模型，对经过word2vec预训练后的词向量根据任务进行微调
- CNN-multichannel：多通道模型，有两套单词向量，每一组向量当成一个通道。

实验证明使用预训练的词向量比基线模型有明显的提高。不过多通道模型对比单通道模型并未有压倒性的优势，有时候多通道模型性能更优，有时候单通道模型占优。

<img src="https://i.postimg.cc/x1rfXVN0/text-CNN-results.png">

### 总结

**向量袋 Bag Of Vectors**

基线模型，表现通常不错。特别是之后接几个relu层。

**窗口模型 Window Model**

适合单字分类等不需要太多背景的问题，例如POS和NER。

**卷积网络 CNN**

适合分类问题。对短文本需要使用padding补齐，较难解释，但是适合在GPU上并行运行。

**时间递归网络 RNN**

并不最适合分类问题。训练符合认知（按顺序读取文本）。训练非常缓慢。适合序列标注。



# 奇淫巧技

### 跳跃和门控

LSTM和GRU中的跳跃和门控思想同样可以运用到其他模型。我们甚至可以在纵向做门控。下图中残差块(Residual block)正运用了这个思想，有效地帮助非常深的NN模型进行训练。右边的高速块(Highway block)看起来功能更加高级，不过实际上效果是等同于残差块的。

<img src="https://i.postimg.cc/HxgqnNqN/residual-highway-block.png" >

### 1*1卷积

1*1卷积也叫网络中的网络(Network-in-network, NiN)连接。卷积核的大小为1。

用处类似于多通道的全连接线性层，将多通道降低成更少的通道。而且参数个数比普通的全连接层要少很多。

### 批量标准

Batch Normalization是CNN中常用技巧，降低模型对参数的初始化的敏感程度，使调整学习率更加简单。在Pytorch中可以使用`nn.BatchNorm1d`。

### 字符级别

上面讨论的基础向量都是词级别的。我们也可以针对字符级别(character-level)的表征来处理类似词性标记(Part-of-Speech, POS)的任务。



# 准递归神经网络

尝试同时吸收RNN和CNN的优点。

<img src="https://i.postimg.cc/dQHCYZZb/QRNN.png">

LSTM的更新如下
$$
z_t = tanh(W_z^1 x_{t-1} + W_z^2x_t) \\\\
f_t = \sigma(W_f^1 x_{t-1} + W_f^2x_t) \\\\
o_t = \sigma(W_o^1 x_{t-1} + W_o^2x_t) \\\\
$$
可并行运行的时序卷积改良成为
$$
Z = tanh(W_z * X)\\\\
F = \sigma(W_f * X) \\\\
O = \sigma(W_o * X)
$$
在池化层内，进行跨通道门控按元素准递归的并行运算。
$$
h_t = f_t \circ h_{t-1} + (1- f_t) \circ z_t
$$
QRNN对语言建模和情感分析有一定程度的性能上的提升，而且速度很快。而且在可解释性上也有进步。

QRNN也有其局限性，在字符级别的语言建模上表现不如LSTM，也许是无法准确捕捉长依赖。





# Reference

- [Dive Into Deep Learning](http://zh.gluon.ai/chapter_recurrent-neural-networks/rnn.html)，第10章

- [Natural Language Processing with Deep Learning](http://web.stanford.edu/class/cs224n/), Stanford University, CS224N, 2019 winter

- [Convolutional Neural Networks for Sentence Classification](https://arxiv.org/abs/1408.5882), Yoon Kim

  