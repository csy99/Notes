# References

- 《自然语言处理技术在业界的应用实践合集》by DataFunTalk
- https://zhuanlan.zhihu.com/p/58931044
- https://zhuanlan.zhihu.com/p/69269702



# Brief History

### NNLM

neural network probabilistic language model. Bengio, 2003. 

简介：三层神经网络模型。用前n-1个词来预测当前词。该模型会学习到（1）词汇的分布式表达（2）通过学习到的分布式表达构建概率函数。

通过这种方式训练出来的语言模型具有很强的泛化能力，对于训练语料里没有出现过的句子，模型能够通过见过的相似词组成的相似句子来学习。

意义：统一了NLP的特征形式embedding。



### Word2vec

Mikolov, 2013. 

简介：线性模型。局部表示（Local Representation）中，每一个元素（如词向量中的一个元素、神经网络中的一个神经元）表示一个实体（词汇）；而在分布式表示（Distributed Representation）中，一个实体（词汇）是由多个元素表征的，同样，一个元素也不仅只涉及到一个实体。基于分布式表示的思想，我们构建一个密集向量（Dense Vector）来表征一个词汇。

CBOW和Skip-gram是Word2vec的两种不同训练方式。CBOW指抠掉一个词，通过上下文预测该词；Skip-gram则与CBOW相反，通过一个词预测其上下文。

意义：不再需要针对特定任务从头训练模型，可以先在大规模预料上进行预训练，再迁移。



### FastText

Mikolov, 2016.

简介：与CBOW模型相似，但有所不同：各个词的embedding向量，补充字级别的n-gram特征向量，然后将这些向量求和平均，用基于霍夫曼树的分层softmax函数，输出对应的类别标签。

引入subword n-gram的概念解决词态变化的问题，利用字级别的n-gram信息捕获字符间的顺序关系，依次丰富单词内部更细微的语义；二是用基于霍夫曼树的分层softmax函数，将计算复杂度从O(kh)降低到O(h log2(k))，其中，k是类别个数，h是文本表示的维数。 



### Transformer

Vaswani, 2017. 

简介：

1. multi-head attention多头注意力

   由scaled dot-product attention单元堆叠而成。把注意力表达成Q(query), K(key), V(value)三元组，分别表示待生成序列、输入序列、和权重。

2. feed-forward network

   通过多头注意力后，经过一个NN，增加网络非线性。添加了像resNet一样的shortcut连接。为了更好地优化深度网络，每个子层都加了残差连接并归一化处理。

3. position encoding

   引入position encoding表达每个词在原始句子中的位置信息。

意义：统一了NLP处理不同任务的模型。支持并行（RNN在t时刻需要依赖t-1的值，很难并行）。解决了长距离依赖。



### BERT

Devlin, 2018. 

简介：采用transformer中encoder的双向结构。在输入端除了有token embedding和position embedding之外，还加上了segment embedding用来处理双句任务。预训练包括两种操作：随机掩盖句子中词语，进行预测；用前一个句子预测下一个句子。

意义：统一了NLP的特征形式embedding。



# 机器翻译

### 简介

编码器+解码器。

注意力模型。多一个或者少一个标点有可能引起译文端的剧烈变化。

### 评价指标

blue

和参考译文对比。n元的串匹配度。

PPL

### 困难点

1. 歧义和未知现象

   黑夜总会过去。新产生的词。

2. 直译和意译

   青梅竹马（不是表面意思）。

3. 翻译解不唯一

   缺乏量化标准

4. 翻译的高度

   和原语言的切合度。意境。



### 交互式翻译

1. 接受用户干预，生成更好的结果
2. 学习用户的修改反馈
3. 实时提供翻译辅助信息

译后编辑、约束解码规则、翻译输入法（打最少的字母输出整句）、在线学习、翻译记忆（需要翻译的句子包含之前人工翻译过的子串）



### 翻译纠错

1. 将语言错误归类，采用Maxent和SVM进行识别
2. 借鉴统计机器翻译的思想，将错误语句翻译成为正确语句

#### 中文错误类型

形似、音似导致的用词错误，多/少字，乱序，常识

#### 关键步骤

**错误检测**

识别输入句子可能存在的问题，采用序列表示（Transfomer)+CRF的序列预测模型

优点：应用句法分析等语言先验知识；结合大量统计特征；多粒度融合

**候选召回**

结合用户历史错误行为和音形等特点进行纠错。可分为离线的候选挖掘和在线的候选预排序。

优点：多来源多粒度

**纠错排序**

基于音形、词法、语义、用户行为等特征学习原词和候选词的多维度距离。保证正确结果的唯一性。

#### 核心技术

**语言知识**

对语言规则的学习。语言模型的训练。

**上下文理解**

需要解决长依赖问题。

**知识计算**

客观规律。



### 机器阅读理解 MRC

给定上下文c和问题q得到答案a。问答的答案线索已经存在于一些文档当中，直接抽取有用信息，提升效率。

#### 关键步骤

**片段定位**

针对问题召回候选文档段落集合。

**输入预处理**

特征预计算。

**在线预测服务**

模型加载和服务驱动。

**后处理机制**

动态规划选取最佳文本短语作为输出。

#### 迁移学习

跨领域：fully shared model + specific shared model

跨语言：小语种数据少。混合语言。



# 知识图谱

### 困难点

1. 知识管理痛点。知识点缺乏关联，需要维护大量相似问题。
2. 语言理解难点。迁移到新领域难以复用原有领域的训练语料。
3. 模型无法进行推理。

### 解决方法

1. 引入知识图谱

   将实体归纳为实体类型。“上海”归纳为城市类。将标准问题归纳成属性。“有多少人口”映射到属性常住人口。抽取实体-属性-值（三元组）。

2. 改进语言理解算法

   semantic parsing算法。包括seq2seq的端到端方案，传统的semantic parsing方案，和多阶段的融合深度学习的semantic parsing方案。

一言以蔽之：将文档数据、用户的查询、存储的答案结构化。



**AMR (Abstract Meaning Representation)**

将句子中的词抽象为概念。最终的语义表达式与原始句没有直接对应关系。

**AMRL (Alexa Meaning Representation Language)**

亚马逊提出。主要应用于任务型场景。

**KAMR (Knowledge-driven Abstract Meaning Representation)**

本质上是有向无环图。主要由Ontology和Language构成。着力解决语言的歧义性、复杂性、复用性、模糊性。

**Semantic Parser**

pipeline。实体识别-属性分类-约束绑定-重排序。每一步都可以比较容易地进行人工干预，精确度高。在实体识别中，可以识别不连续的序列标签（专有名词被其他修饰语隔开，比如：“大流量58元套餐”中的“58元”）。属性识别就是意图分类，其中的查询包括(domain, predicate, target, query type)。



## 构建

### 构建途径

#### 知识表示

知识图谱的过程、时空、多模态语义增强。单纯基于符号去理解世界的能力是有限的。要把图片、声音、视频化的认知体验与相应符号结合。

与其他知识表示的协同机制与办法。很多领域没有太多数据和知识图谱，但是积累了丰富的专家知识，通常是规则库。知识图谱可以和这些规则库进行模型整合。

个性化表示。原生的图模式，或者RDF（resource description framework)，或者主谓宾的形式表示。人具有主观性，需要合理控制不同视角下的不同图谱。比如龙在东方多为褒义而在西方则带有凶恶的贬义。

#### 知识补全

利用已有的知识进行反推。比如A的老公是B，可以推出B的老婆是A。

低成本知识获取机制，主要指无监督或者弱监督、小样本。

多粒度知识获取。支撑不同级别的应用。

大规模学习常识。发展瓶颈在于常识。人人都知道，所以不会明说，这些知识不会存在与数据中，机器难以学习。

#### 知识扩展

将知识补全到图谱中。主要是一些新产生的词、代号等等。也包括从用户的对话中抽取信息，比如用户说明天是我的生日，可以知道用户的生日。

#### 知识更新

状态的更新。比如某个公司CEO卸任。

#### 应用透明化

用户的参与其实就是最大成本。模型学习到的无数隐式特征，能不能用图谱中的符号化的知识、路径或者概念去解释。

### 隐变量模型 LatentFactorModel

LFM模型提出基于关系的双线性变换，刻画实体和关系的二阶联系。评分函数：$f_r(h,t) = l_h^TM_rl_t$。其中M是关系r对应的双线性变换矩阵。

### 张量神经模型 Neural Tensor Network

用双线性张量取代传统NN中的线性变换，在不同维度下将头、尾实体向量联系起来。

**缺点**

1. 计算复杂度高
2. 需要样本多
3. 大规模稀疏知识图谱上的效果比较差

### 社区发现

#### Louvain

社区发现算法。尽可能提升模块度（衡量社区紧密度的标准）。希望社区内边多，外边少，即社区更聚集。



## 对话平台

架构图：自然语言理解NLU -> 对话管理DM -> 自然语言生成NLG

### NLU

#### 情况

无样本、小样本、多样本

#### 小样本处理方案

Memory-based Induction Network

模仿人类的记忆和类比能力。

Induction Network

把样本向量抽象到类向量。

### DM

#### 简介

1. 业务建模：对不同行业不同场景的业务进行抽象。用户说、机器思考、机器说分别对应三个基础节点：触发节点、函数节点、回复节点。设计双层状态机，上层是对话逻辑，底层是一套通用的对话引擎（解耦）。
2. 鲁棒性：应对未定义的通用对话需求和异常情况，包括打断恢复、模糊澄清、个性化拒识、信息修改。
3. 持续学习的能力：在交互中学习，根据用户反馈调整对话策略。

#### Dialog State Tracking (DST)

1. 多智能体建模：除了跟用户交互，还跟多个外部服务交互
2. 追踪变量：外部返回的结果会存储在变量中
3. slot-value假设：不一定离散可枚举
4. 追踪次数：不限制

#### Dialogue Policy Optimization (DPO)

给用户推荐排名最高的结果。

### NLG

#### 用户模拟器

有三部分组成：User State Tracker, User Policy，和User Model。

#### 对话诊断

利用一个机器人诊断另一个机器人。

### 类型

**任务型对话 TaskBot**

询问天气、订票服务。

**问答机器人QABot**

主要是一问一答。但是也有多轮对话。

基于知识图谱的KBQA。

基于非机构化的网页只是来抽取答案的DeepQA。

**闲聊机器人ChatBot**

技术上难度最高。主要有检索式和基于深度学习的生成式。检索式对话模型流畅性好，生成式对话模型多样性强。

### 知识结构化

#### 模式层

定义类型、属性、值和关系。属性支持子属性（层级关系）。值可以支持多种类型，比如text, key-value, 和compound value type。

#### 实体层

对模式层的实例化。“实体-属性-值”或者“实体-关系-实体”进行建模。

#### 领域词（短语挖掘）

Wide统计特征:  频率、凝聚度、自由度

Deep语义特征：字向量、词向量、基于词向量的内部相似度

#### KBQA

knowledge based question answering。着重解决实体类知识回答。

#### EBQA

Event-Based Question Answering. 除了给出精确答案，还需要解释为什么。支持假设性的事件问答。

事件识别  -> 属性识别。

### 提高对话质量

万能回复没有信息量。通过人工设置一些控制技术，使回答包含指定内容。

使用attention结构解决多轮聊天上下文无关的问题。

多轮对话过程是动态变化的，不能用单轮的贪心方式来建模多轮过程。

引入个性化信息提高对话的多样性。机器人说话风格可以存在差异。



## 查询理解

#### 难点和解决方案

1. 输入错误。纠错模块。
2. 表达冗余。计算每一个词的重要程度term weight。
3. 存在语义鸿沟。需要对原始query做同义词拓展。

### 召回系统

基于词的传统的倒排索引召回

基于向量的向量索引召回

#### Query Term Weight

IDF词典+语境动态调整。

#### 同义词拓展

同义词词表。训练word embedding模型，利用embedding的值计算相关度。

#### Query改写

将用户的查询，改成更容易检索出答案的query，减小多个任务的损失。把改写问题看成是src和target是同一种语言的翻译问题。

传统的基于上下文的query改写包括两种方式：(1)将上下文关键词抽取出来，和当前查询做一个组合；这种方法会对序列产生较大影响，可能会忽略某些信息，比如否定类的词。(2)通过sequence建模，把上下文和query放到统一模型中得到向量；这种方法需要的计算资源很大。

**步骤**

1. 使用pointwise mutual information算法根据查询和回复，抽取上下文中与其共现概率最大的若干词作为关键词
2. 使用语言模型将这些信息插入到query中，计算插入不同位置的得分，使用beam search生成分数较高的句子
3. 使用复制网络copy net来学习回复的先验知识，改善模型训练
4. 使用强化学习

## 意图理解

了解客户需求（意图识别），细分到不同业务领域（属性抽取）。如果领域不能明确，可以跟用户进行一轮澄清。

与槽填充不是相互独立的，可以利用共有的知识，应该进行联合建模。

### 模型迭代

1. 搜集业务数据，打标签
2. 模型训练
3. 对样本进行预测
4. 对也测样本进行标注，选出错误和难分开的样本
5. 重新训练

### 情绪回复能力

理解情绪+表达情绪

对最常用的7类情绪（委屈、恐惧、着急、失望、愤怒、辱骂、感谢）模型训练出单独的分类模型。

还可以做客服质量检测（有没有反怼、讽刺客户的现象）。

整段对话的标签可能不适用于整段对话的其中一部分（投诉电话不一定全程激动）。弱标签学习。

### 对话设计

1. 问问题的方式很重要
2. 如果有可能，多提供一些信息，但是不要太过
3. 使用个性化信息：用户地理位置
4. 让机器人有记忆



## 信息抽取

将非结构化的文本信息转换成为结构化的信息。主要包括命名实体识别(NER)、关系抽取(RE)、事件抽取。

### 命名实体识别(NER)

主要用RNN进行encode。在预测的时候，用CRF做序列标注。

### 关系抽取(RE)

两种思路。第一种是pipeline，先进行NER，在进行RE。第二种是joint，通过共享参数，把两者联系在一起。encode既包含CNN相关，也包含RNN相关。

难点：

1. 处理层级关系
2. 准确率问题

### 步骤

1. 问题编码。问题使用双向lstm并加入答案的attention编码。
2. 答案编码。答案本身编码，答案到实体的关系编码，答案类型编码，答案上下文编码。

### 挑战

#### 实体爆炸

将实体与用户资源关联起来，并且把实体汇聚起来。

标品：区别不同商品类别的属性。（条形码、名称、规格）

同构非标品：相同本体下要求某些特定属性相同（青岛啤酒）。

易购纯概念：先天并不存在，完全由人类自行组织构建从而形成的认知（北京一日游）。

#### 非连通查询

存在属性传递，也就是传递约束条件。例子：有不有下午4点复联四的票？（拍片时间并不是电影的一个直接属性）。

使用实体关系路径游走。在离线状态下将这些关系补全，减少之后查询的压力。

#### 融合上下文

改造理解层、生成层和输出层适配多轮交互模块。解耦KBQA内部组件使之可独立被外部模块调用。



## 范式

### 范式1

Embed：单词、字符、词性、位置。

Encode：CNN，RNN，Transformer

Attend：sum, product

Predict：classification, seq labeling, seq2seq(翻译、摘要)

### 范式2

pre-train: BERT等算法在大数据量上进行预训练。

fine-tune: 针对特定任务进行微调。

Predict：classification, seq labeling, seq2seq(翻译、摘要)。





# 信息流推荐

### 历史

门户时代。数据较少，可以通过人工对内容类型进行整理，以频道页的形式满足用户需求。

搜索/社交时代。搜索要知道文章是关于什么的，可以用关键词进行解决。难以克服实体歧义问题。知识图谱部分解决了这一问题。

智能时代。个性化推荐。推荐和搜索的区别。推荐需要说清楚文章是是什么，如果仅仅以词粒度作为用户兴趣点，会缺少原文此关系。另外推荐需要了解为什么，要有推理的能力。

### 简介

item的维度：语言识别、质量控制、时效控制

用户兴趣：分类、标签、关键词、主题

热点挖掘：多源新闻热点挖掘

### 难点

1. 语言多
2. 任务多
3. 样本少
4. 人才少

### 思路

把语言相关的在NLP中解决，下游一系列任务可以去语言化，算法能够通用。在样本迁移中，把小语种翻译为三大语种，用大语种模型做分类。

MUSE：多源词向量对齐方法。

基于BERT的迁移文本分类。先用大规模的单一语料做预训练，再接一些任务的调整。

ML-SSC (multi-label semi-supervised clustering)。弱样本依赖模型。零样本要求，能够快速通用覆盖，且泛化性好。但是细标签不够准确。

做不依赖于NLP基础建设的用户显示的兴趣挖掘。

简历隐式用户画像。

### 兴趣点图谱

#### 分类层

一般由PM建设，树状结构。解决了人工运营需求。

#### 概念层

有相同属性的一类实体称之为概念，比如老人机。推理用户消费的真实意图。

#### 实体层

知识图谱中的实体，比如iphone11。兴趣点的召回。

#### 事件层

刻画某一个事件，比如iPhone 11发布会。刻画文章内容。

### 热门事件挖掘

根据事件搜索量变化趋势判断。Burst Region Detection (BRD)判断时间序列上面是否有爆发点。

可以用DTW算法进行热门识别。

### 关联关系

如果两个实体经常在同一篇文章中，很有可能是高关联的。但是没有共现过的，也有可能是高关联的。共现的作为正例，负样本采用同类实体随机负采样。正负样本比例1:3。



# 虚拟生命

加入多模态交互：语音、手势、视觉。

### 个性化语音合成

前处理，端到端的深度学习算法，vocoder技术，后处理。汉语多种音调的处理。

### 人脸重建

分两个工作流。上面做3D模型，下面做shader渲染。



# 视频内容理解

### 细粒度视频内容分割

把场景、前景、片尾、字幕、音乐分开。
logo识别、字幕检测

### 视频标签

场景和物品。事件和行为。

定义一套Taxonomy分类标准。在给定数据源生成标签，再进行处理。做多个算法的模块质量评估。模型融合。

### 内容生成

精彩片段推荐。视频摘要。探索性项目。

### 内容向量

实际上是创造一个embedding。



# 模型性能问题

### 离散且大小不定的用户请求

不易batch运算。

解决方案：service streamer。将服务请求排队组成完整batch。

### 





# 学术和业界区别

|          | 学术               | 业界             |
| :------- | ------------------ | ---------------- |
| 问题定义 | 明确               | 模糊             |
| 数据     | 规范               | 脏               |
| 模型     | 优雅、创新         | 需要制定人工规则 |
| 评价指标 | 比较固定           | 以产品为导向     |
| 工程     | 调试到结果满意即可 | 不断迭代         |

